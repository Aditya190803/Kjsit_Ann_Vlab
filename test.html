<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Experiment 2: Implementation of Activation Functions in Artificial Neural Network</title>
    <style>
        /* Add your CSS styles here */
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
            padding: 20px;
        }
        h1, h2, h3 {
            color: #333;
        }
        p {
            color: #666;
        }
        .section {
            margin-bottom: 40px;
        }
        .section-title {
            font-size: 24px;
            font-weight: bold;
            margin-bottom: 10px;
        }
        .sub-section {
            margin-left: 20px;
        }
    </style>
</head>
<body>
    <header>
        <h1>Experiment 2: Implementation of Activation Functions in Artificial Neural Network</h1>
    </header>

    <div class="section">
        <h2 class="section-title">Theory</h2>
        <p>The activation function is used to calculate the output response of a neuron. The sum of the weighted input signal is applied with an activation to obtain the response. Different activation functions have different properties:</p>

        <div class="sub-section">
            <h3>1. Binary Step Activation Function:</h3>
            <p>Binary step function is a threshold-based activation function which returns '0' if the input is less than zero, and '1' otherwise.</p>
            <!-- Insert Fig. 2.1 Binary step activation function -->
            <!-- Insert code or image here -->
        </div>

        <div class="sub-section">
            <h3>2. Linear Activation Function:</h3>
            <p>The Linear Activation Function returns the input as it is. It is represented by f(x) = x.</p>
            <!-- Insert Fig. 2.2 Linear activation function -->
            <!-- Insert code or image here -->
        </div>

        <!-- Add other activation functions and descriptions similarly -->

    </div>

    <div class="section">
        <h2 class="section-title">Program</h2>
        <p>Below is the Python code to implement different activation functions:</p>

        <pre><code>import numpy as np 
import matplotlib.pyplot as plt 

# Binary Step Activation Function 
def binaryStep(x): 
    return np.heaviside(x, 1) 

x = np.linspace(-10, 10) 
plt.plot(x, binaryStep(x)) 
plt.axis('tight') 
plt.title('Activation Function: Binary Step') 
plt.show() 

# Linear Activation Function 
def linear(x): 
    return x 

x = np.linspace(-10, 10) 
plt.plot(x, linear(x)) 
plt.axis('tight') 
plt.title('Activation Function: Linear') 
plt.show() 

# Sigmoid Activation Function 
def sigmoid(x): 
    return 1 / (1 + np.exp(-x)) 

x = np.linspace(-10, 10) 
plt.plot(x, sigmoid(x)) 
plt.axis('tight') 
plt.title('Activation Function: Sigmoid') 
plt.show() 

# Bipolar Sigmoid Activation Function 
def bipolarSigmoid(x): 
    return -1 + 2 / (1 + np.exp(-x)) 

x = np.linspace(-10, 10) 
plt.plot(x, bipolarSigmoid(x)) 
plt.axis('tight') 
plt.title('Activation Function: Bipolar Sigmoid') 
plt.show() 

# Tanh Activation Function 
def tanh(x): 
    return np.tanh(x) 

x = np.linspace(-10, 10) 
plt.plot(x, tanh(x)) 
plt.axis('tight') 
plt.title('Activation Function: Tanh') 
plt.show() 

# ReLU Activation Function 
def relu(x): 
    return np.maximum(0, x) 

x = np.linspace(-10, 10) 
plt.plot(x, relu(x)) 
plt.axis('tight') 
plt.title('Activation Function: ReLU') 
plt.show() 

# Leaky ReLU Activation Function 
def leakyRelu(x): 
    alpha = 0.1 
    return np.maximum(alpha * x, x) 

x = np.linspace(-10, 10) 
plt.plot(x, leakyRelu(x)) 
plt.axis('tight') 
plt.title('Activation Function: Leaky ReLU') 
plt.show() 

# Softmax Activation Function 
def softmax(x): 
    return np.exp(x) / np.sum(np.exp(x), axis=0) 

x = np.linspace(-10, 10) 
plt.plot(x, softmax(x)) 
plt.axis('tight') 
plt.title('Activation Function: Softmax') 
plt.show()</code></pre>

    </div>

    <div class="section">
        <h2 class="section-title">Outputs</h2>
        <p>Outputs of the implemented activation functions:</p>

        <!-- Insert outputs here -->

    </div>

    <div class="section">
        <h2 class="section-title">Conclusion</h2>
        <p>The activation functions play a major role in determining the output of neural networks. Different activation functions have different properties and are suited for different types of problems.</p>
    </div>

    <footer>
        <p>Text/Reference Books:</p>
        <ul>
            <li><a href="#">Neural Network a Comprehensive Foundation by Simon Haykin</a></li>
            <li><a href="#">Introduction to Soft Computing by Dr. S. N. Shivnandam, Mrs. S. N. Deepa</a></li>
            <!-- Add other books similarly -->
        </ul>
        <p>Web Addresses (URLs):</p>
        <ul>
            <li><a href="https://en.wikipedia.org/wiki/Activation_function">Wikipedia: Activation function</a></li>
            <li><a href="https://analyticsindiamag.com/most-common-activation-functions-in-neural-network">Analytics India Mag: Most common activation functions in neural network</a></li>
            <!-- Add other URLs similarly -->
        </ul>
    </footer>
</body>
</html>
